# Practice Exam Questions

## Network Design Questions
* You have recently launched a brand new application with multiple EC2 instances that sit behind an ELB. Yo uhave setup autoscaling to scale the EC2 instances from 5 to 35 instances. You have been notified by the security team that you need to implement HTTPS on the application when client access it over the internet. You must implement this in the shortest time possible. What solution would you consider and implement?
  A: Use ACM to install a SSL certificate onto the ELB. The easiest way to apply a secure connection for your clients is to use SSL termination at the ELB. This will allow users to use HTTPS when accessing your application.
* A global telehealth company is planning on updating its patient health system. There are multiple business units within the company, each with their own specific focus in healthcare. Each of these business units develops and runs applications that are hosted in their own individual AWS accounts within their own separate application VPCs. All of the VPCs are in the same AWS Region. The applications are configured to retrieve data from a centralized shared services VPC within the organization. The company goal is to create a network connectivity architecture that offers detailed security controls. Additionally, this architecture must be scalable to accomodate potential for many more business units in case of company growth. What solution will satisfy these requirements in the MOST secure way?
  A: Construct VPC endpoint services utilizing AWS PrivateLink within the central shared services VPC. Deploy VPC endpoints in every application VPC for the required services. You can host your own AWS PrivateLink powered service, known as an endpoint service, and share it with other AWS customers. Endpoint services allow for maximum security controls via endpoint policies, NLB security groups and NACLs as well as controlling which consumer VPCs the endpoints get deployed to. AWS PrivateLink ensures that traffic between VPCs and services remains on the Amazon network. This minimizes exposure to the public internet, thus enhancing security. As more business units consume data from the central shared services VPC, more endpoints can easily be added, meeting the requirement for scalability.
* You have been tasked with setting up a static S3 website to serve out to a large variety of users spread out throughout the globe. Most of your users will be in the Americas and the European regions. You launched the static site in a S3 bucket in the us-east-1 region. You are configuring CloudFront to serve the content and need to ensure that performance is high and the cost is as low as possible. What should you do?
  A: Configure CloudFront to use the price class of the United States, Canada, and Europe regions. CloudFront has edge locations all over the world. Our cost for each edge location varies and, as a result, the price that we charge you varies depending on the edge location from which CloudFront serves your requests. The least expensive price class inclues (the United States, Canada, and Europe regions). This will not only be lowest cost but also highly performant.
* Your organization has an on-premises network and is using EC2 instances in two different Regions to replicate data over the internet. Traffic analysis reveals that the MTU remains at the default value of 1500. What can be done to increase the MTU value in order to improve data transfer time?
  A: Use Jumbo frames for traffic between your VPCs and your on-premises networks over AWS Direct Connect. Use AWS Direct Connect and route packets between the VPCs using Jumbo Frames. Private and Transit Virtual Interfaces support a jumbo frames option, with a maximum MTU of 9100 for Private VIFs, and 8500 for Transit VIFs. While a highly impractical solution in this scenario - as direct connections between VPCs cannot be accomplished through AWS Direct Connect, requiring traffic from each VPC to pass through your routers at your on-premises network - It is the only option that allows for a higher MTU value with causing fragmentation. Direct Connect with Public VIFs, all traffic over the internet - even in a VPN, and connections across VPC peering have a maximum MTU of 1500. While enabling enhanced networking can increase network throughput within a VPC, it makes no change to the MTU used by an EC2 instance. Changing an EC2 instance MTU is a separate configuration.
* A corporation utilizes a hybrid architecture, with an AWS Direct Connect connection linking its on-premises data center to AWS. Various applications send traffic between the on-site data center and a VPC. Domain names are configured for both the on-premises and VPC resources. They use the corp.internal.net for the on-premises domain name, and they use aws.internal.net for the AWS VPC via a Route 53 Private Hosted Zone. Currently, they are running an EC2-hosted DNS resolver in the VPC that forwards request from AWS to another on-premises DNS resolver for any corp.internal.net, and vice versa for aws.internal.net traffic. The company plans to switch from using the DNS resolver to Amazon Route 53 Resolver endpoints only within the VPC. What three steps should a network engineer follow to complete this migration?
  A: Configure the on-premises DNS resolver to forward aws.internal.net DNS queries to the IP addresses of the inbound endpoint, which will use the Route 53 resolver to resolve VPC Private Hosted Zone records. To forward DNS queries from your network to Resolver, you create an inbound endpoint. An inbound endpoint specifies the IP addresses (from the range of IP addresses available to your VPC) that you want DNS resolvers on your network to forward DNS queries to. Those IP addresses aren't public IP addresses, so for each inbound endpoint, you need to connect your VPC to your network using either an AWS Direct Connect connection or a VPN connection. Reference Documentation: Forwarding Inbound DNS Queries to Your VPCs
  A: Create a Route 53 Resolver inbound endpoint and a Route 53 Resolver outbound endpoint. Amazon Route 53 Resolver responds recursively to DNS queries from AWS resources for public records, Amazon VPC-specific DNS names, and Amazon Route 53 private hosted zones, and is available by default in all VPCs. Before you start to forward queries, you create Resolver inbound and/or outbound endpoints in the connected VPC. These endpoints provide a path for inbound or outbound queries. Reference Documentation: Resolving DNS Queries between VPCs and Your Network.
  A: Create a Route 53 Resolver rule to forward corp.internal.net DNS queries to the IP address of the on-premises DNS resolver. Creating a Route 53 Resolver rule to forward corp.internal.net queries to the IP address of the on-premises DNS resolver ensures that queries for the on-premises resources will be directed to the correct destination in the on-premises data center.
* A company's web application hosted on AWS employs an Application Load Balancer (ALB) that is running in multiple Availability Zones, and using AWS Lambda functions as the targets. Amazon CloudWatch metrics are being used for monitoring purposes. Some users have complained about parts of the web application not loading as they should, and a network engineer is tasked with investigating the issue. After enabling access logging for the ALB, what action should the network engineer take to identify the errors that the ALB is encountering?
  A: Set up an Amazon S3 bucket as the destination for the access logs. Utilize Amazon Athena to identify the error messages that the ALB is experiencing by querying the logs in S3. Application Load Balancers provide access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. After you enable access logs for your load balancer, Application Load Balancers capture the logs and stores them in the Amazon S3 bucket that you specify as compressed files. When you enable access logs for your load balancer, you must specify the name of the S3 bucket where the load balancer will store the logs. The bucket must have a bucket policy that grants Elastic Load Balancing permission to write to the bucket. Amazon Athena is a query service that makes it easy to analyze data in Amazon S3 using standard SQL statements. You can easily point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds. This leaves the data unaltered as well.
* You work for a federal organization that has various restrictions on who can access their CloudFront content. The application is running on a Tomcat web server that is sitting behind a load balancer. How can you restrict certain locations from accessing the content?
  A: Take advantage of the X-Forwarded-For HTTP header and then restrict content via CloudFront geo restrictions. You can use the X-Forwarded-For header to capturing the requestors source location. You can use this location in conjunction with CloudFront geo-restriction to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution.
* Your startup operates several workloads running on Amazon EC2 instances that are deployed in public subnets. During a recent event, a bad actor took advantage of a vulnerability in one of your applications and gained access to an instance. Your company addressed the issue in the application by updating the vulnerable code and then replaced the compromised EC2 instance with another instance that had the updated software. The bad actor exploited the compromised application to distribute malware across the internet, and the company was alerted to the breach through a notification from AWS. Otherwise, they had no idea it was happening. Your company now requires a mechanism to detect when an application deployed and running on an EC2 instance is potentially spreading malware. What solution will fulfill this need with the MINIMUM operational effort?
  A: Utilize Amazon GuardDuty for assessing traffic trends by examining DNS queries and VPC Flow Logs. Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. It automatically analyzes DNS requests and VPC Flow Logs, and there's no need for extensive configurations or ongoing management. By inspecting DNS requests and VPC Flow Logs, GuardDuty can detect unusual patterns and possible threats, including malware spreading activities. GuardDuty also offers Malware protection, which can be triggered via detection of specific EC2 findings (list below).
* You are a solutions architect for an application team within a fast growing startup. The team is launching a new multi-tier application on AWS, and it is going to run on several Amazon EC2 instances within an Auto Scaling group fronted by a publicly accessible Network Load Balancer (NLB). The application must handle both UDP and TCP traffic and initially serve users within the same geographic location, eventually expanding to a global audience. To achieve this expansion and control traffic distribution during new version rollouts, while also minimizing latency and jitter for global end users, the application team is considering the following design points for the network architecture:
  A: Create an AWS Global Accelerator endpoint, and define listeners for the necessary ports. Set up endpoint groups for each Region, and leverage a traffic dial for the endpoint groups to control the amount of traffic towards the latest Regional deployments. Link the Network Load Balancers (NLBs) to the endpoint groups. AWS Global Accelerator is a service that improves the availability and performance of applications with local or global users. It uses static IP addresses that act as a fixed entry point to your application endpoints, allowing traffic to be routed to optimal Regions. Endpoints for standard accelerators can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses. The application requires support for both UDP and TCP traffic (which is a major key indicator in this scenario), and Global Accelerator can handle both protocols. By configuring endpoint groups for each AWS Region, the application can distribute traffic across multiple geographically dispersed endpoints. Global Accelerator provides a traffic dial feature that allows you to control the percentage of traffic directed to an endpoint group. This is especially helpful when rolling out new versions of the application across different Regions. By registering the Network Load Balancers (NLBs) with the endpoint groups, Global Accelerator can distribute traffic over multiple AWS endpoints in one or more AWS Regions. This helps in minimizing first-byte latency and jitter for the end users, as required.
* Your company has a new application being launched that takes advantage of high performance computing nodes. The nodes that are used will be processing large amounts of high definition computer generated images (CGI). Which of the following should be taken into considerations when designing the architecture?
  A: Consider launching the instances in a cluster placement group. The G3 instances types are used for graphic intensive workloads and also are supported for enhanced networking. We can also take advantage of cluster placement groups to provide low-latency and high-throughput for our HPC workload.
  A: Consider using G3 instance types. The G3 instances types are used for graphic intensive workloads and also are supported for enhanced networking. We can also take advantage of cluster placement groups to provide low-latency and high-throughput for our HPC workload.
* There has recently been an outbreak of a contagious virus leading your organization to force employees to work from home. Normally the employees use machines that are on-premise but it is your job to setup an Amazon WorkSpace environment to make sure employees are able to use and access the applications served internal from your organization. You have a need to setup 1000 Amazon WorkSpaces. You are designing the minimal architecture to handle the new workload, what would you need to create?
  A: Two subnets each with a /22 subnet mask. You'll need a minimum of two subnets for a WorkSpaces deployment because each AWS Directory Service construct requires two subnets in a Multi-AZ deployment. A /20 provides four times the number of IP addresses required. The only answer that would work and meet the minimal architecture approach is two subnets each with a /22 subnet mask.
* Your company is aiming to transfer its DNS registrar and DNS hosting to Amazon Route 53, as the existing DNS provider is unable to handle the tens of thousands of daily visits to the company's website. The company is seeking to make this migration as fast as possible without having any service interruptions. What solution will best meet these conditions?
  A: Copy any and all of the DNS records from the existing DNS provider to a Route 53 public hosted zone. Then, update the name servers with the existing registrar to point toward the Route 53 name servers for the newly created public hosted zone. After all changes have propagated successfully, finish the process by starting a domain name transfer to Route 53. Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. You can use Route 53 to perform three main functions, in any combination: domain registration, DNS routing, and health checking. This process will offer a solution with no interruption to DNS resolution of the backend resources by leveraging the process known as domain delegation. By modifying the existing NS records in the original DNS provider's settings, you can quickly point to the Route 53 public hosted zone in order to leverage Route 53 for its DNS capabilities. This delegates control of the domain name space to Route 53, which is why you can then also copy existing DNS records into the public hosted zone for resolution. Once records are resolving correctly via Route 53, the domain name transfer allows you to begin fully managing the DNS zone, all from with Route 53, with no downtime.
* Your organization hosts several departmental VPCs across multiple AWS regions. All departmental VPCs require access to services placed in a shared VPC. Partner companies need to connect their VPCs to the shared services VPC as well. Individual Department VPCs and partner company VPCs should not connect to each other. Identify each independent solution.
  A: Create a VPC peering connection between all departmental VPCs and the shared services VPC. Establish VPC peering connections between your partner VPCs and the shared services VPC too. All options will allow connectivity, but the transit VPC option fails to restrict communication between departmental and partner VPCs. Unless BGP route learning is carefully managed, default dynamic routing configuration could share prefixes to other VPCs.
  A: Provide access to applications in the shared services VPC via Endpoint Services. All options will allow connectivity, but the transit VPC option fails to restrict communication between departmental and partner VPCs. Unless BGP route learning is carefully managed, default dynamic routing configuration could share prefixes to other VPCs.
  A: Attach Internet Gateways to all VPCs, And allow access to the shared services over the Internet. Use security and authentication controls as necessary to regulate service access from the other VPCs. All options will allow connectivity, but the transit VPC option fails to restrict communication between departmental and partner VPCs. Unless BGP route learning is carefully managed, default dynamic routing configuration could share prefixes to other VPCs.
* A corporation operates 20 web server Amazon EC2 instances within an Auto Scaling group in their application VPC and 20 additional web servers running within their on-premises data center. They have established a single 10 Gbps AWS Direct Connect link between the on-site data center and the application VPC. The business needs to deploy a load balancing solution capable of handling HTTPS traffic from hundreds to thousands of external users on the internet. This solution must evenly distribute the traffic between the AWS web servers and the on-premises web servers. During a user's entire session, HTTPS requests must be directed to the same web server, regardless of if the server is in AWS or on-premises. What approach will fulfill these needs?
  A: Create an Application Load Balancer (ALB) inside the application VPC within a public subnet, and create then a target group. Select ip as the target type. Register the EC2 instances and on-premises servers with the target group. Enable session affinity (sticky sessions) on the ALB. An ALB operates at the application layer (Layer 7) of the OSI model and is capable of handling advanced routing, including session affinity (sticky sessions). This ensures that all requests from a client during a session are sent to the same server, which meets the requirement specified. By specifying IP as the target type, you can register both the EC2 instances in AWS and the on-premises servers as targets. This allows the ALB to distribute traffic across both AWS VPCs and on-premises servers, meeting the company's need. Enabling application-based session affinity ensures that all requests from a user during a session are routed to the same server, whether it's an EC2 instance or an on-premises server.
* You have an application that is running on Amazon EC2 instances within an Auto Scaling group running in us-east-1a. The application is running behind a public Network Load Balancer (NLB). To enhance the application's availability, you have decided to add EC2 instances into Availability Zone us-east-1c, and you have added them to the existing target group. However, you have realized that the traffic is being directed solely to the instances in the us-east-1a Availability Zone. What would be the MOST efficient method to fix this issue?
  A: Make sure to enable the us-east-1c Availability Zone for the NLB. You enable one or more Availability Zones for your load balancer when you create it. If you enable multiple Availability Zones for your load balancer, this increases the fault tolerance of your applications. You can't disable Availability Zones for a Network Load Balancer after you create it, but you can enable additional Availability Zones. Since you were running workloads in us-east-1a only initially, you would need to enable the second AZ in order for the targets to actually begin receiving traffic.

## Network Implementation
* You have set up a number of rate-based rules that are used across your WAF ACLs. Your Chief of Network Security wants to be immediately notified when certain rate-based rules are triggered and have the IP addresses that were blocked. Which of the following options will meet this requirement with the least amount of effort?
  A: Create a CloudWatch Logs log group, enable logging in AWS WAF, and specify the log group ARN. Configure Amazon Simple Notification Service notifications for detected Amazon CloudWatch alarms and send out the needed alerts. This method would be work and require the least amount of effort. AWS WAF supports the following log destinations: a CloudWatch log group, an S3 bucket, or an Amazon Kinesis Data Firehose destination.
* You have been tasked with creating a cloud formation template which will be used as a baseline for developers within your organization. Using your current version of a template, you have created a simple VPC within your home Region, containing public & private subnets in a single AZ, an Internet Gateway, and a NAT Gateway. The template that you produce must be usable in any Region. In addition, for the sake of clarity, the VPC ranges selected by the developers must conform to a particular CIDR pattern. Which of the following can be used to facilitate these requirements with minimal effort?
  A: Use Parameters to allow developers to specify a unique portion of the required CIDR pattern. You should use Parameters to allow developers to specify a unique portion of the required CIDR pattern. As the CIDR pattern to be used by developers is already pre-defined, you should not use Parameters to allow developers to specify the VPC CIDR. The Fn::Cidr intrinsic function could certainly be used to derive IP ranges for subnets within a VPC, but it needs an initial CIDR block to be provided, so it cannot be the single correct answer. The optional Conditions section contains statements that define the circumstances under which entities are created or configured. Objects may reference a condition in their properties, and if the identified condition is not true, then the object will not be created. While this could be used to eventually roll back stack creation if required objects cannot be created, it is a much more complicated process to accomplish the objective.
* You work for an organization who serves out client authentication services to their end users. For compliance reasons, these client authentication services rely on certificates that live on each of the EC2 instances to decrypt and encrypt the data. You've been tasked with setting up an elastic load balancer to provide end-to-end encryption. Which of the following options would provided end-to-end encryption?
  A: Set up a Classic Load Balancer that provides TCP/443 passthrough. To provide end-to-end encryption, the EC2 instances must have certificates installed onto them. Since in this scenario, we are explicitly not using SSL/TLS offloading, then the encryption and decryption will be done by the EC2 instances. We can use a TCP passthrough from a Classic or Network Load Balancer.
* Your organization wants to maintain tight control over the types of resources provisioned within production VPCs Located in the home AWS Region. It has been determined that newly created compute resources that are not in compliance with production environment policies should be immediately terminated, and internal administration should be notified of the event. Which of the following configurations can satisfy the objectives with the least amount of administrative effort?
  A: Create a Lambda function which contains code that identifies non-compliant resources and triggers notification via SNS. Reference the function with an AWS Config custom rule, and create a remediation action which automatically attempts to terminate instances identified by the lambda function. AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. While AWS Config provides managed rules that allow some customization, you can develop custom rules by associating an AWS Lambda function containing the code that evaluates whether your AWS resources comply with the rule. To apply remediation on noncompliant resources, you can either choose the remediation action you want to associate from a prepopulated list or create your own custom remediation actions using SSM documents. While all of the other options could be used as part of the remediation process, they would take greater administrative effort to implement.
* You are setting up a new VPC with a few different subnets. The resources that are launched in the VPC will need to retrieve custom information from a NetBIOS name server that has already been setup. The IP address has been provided to you. Which of the following will provide a solution with the MINIMUM effort?
  A: Setup a custom DHCP option set and provide the NetBIOS name server IP address. The Dynamic Host Configuration Protocol (DHCP) provides a standard for passing configuration information to hosts on a TCP/IP network. The options field of a DHCP message contains the configuration parameters. Some of those parameters are the domain name, domain name server, and the netbios-node-type. You can configure DHCP options sets for your virtual private clouds (VPC).
* Your organization is evaluating requirements virtual desktop application infrastructure. It has been determined that common applications should be supported using AWS services. Users do not need a persistent desktop environment across multiple work sessions. Resources should scale according to a schedule, but also by capacity during periods of high demand. Which AWS service can best provide this functionality?
  A: Amazon AppStream 2.0 is a fully managed application streaming service that provides users with instant access to their desktop applications from anywhere. AppStream 2.0 manages the AWS resources required to host and run your applications, scales automatically, and provides access to your users on demand. Amazon WorkSpaces enables you to provision virtual, cloud-based Microsoft Windows or Amazon Linux desktops and provides a persistent desktop experience for your users. Environments provisioned using AWS OpsWorks would be on EC2 instances. Using individual EC2 instances for each employee would likely be much more expensive and less controllable than Amazon AppStream 2.0.
* Your company is using Elastic Beanstalk environments to facilitate their blue-green deployment strategy. Testing has been completed. Route 53 health checks have been configured for all instances, weights have been re-configured on all records, and the green environment is now handling 100% of the request traffic. However, in a terrible mishap the CloudFormation stack containing the green environment has accidentally been deleted, and your new web server instances are currently being terminated. Until the green environment has been recreated, what will happen to incoming requests for your application?
  A: Requests will be sent to the blue environment. Presuming it has not already been deleted, requests will be sent to the blue environment. Regarding health checks and weighted record sets, if you add health checks to all the records in a group of weighted records with non-zero weights on some records and zero weights on others, Route 53 initially only considers the health of non-zero weighted records. If all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.
* You need to ensure that resources in the VPC can resolve both AWS and on-prem DNS resources. How can you achieve this?
  A: Provision EC2-hosted DNS servers in your VPC and define rules to regulate which DNS system requests are forwarded to. Provisioning EC2-hosted DNS servers and defining forwarding rules can facilitate the resolution of DNS queries between AWS and on-premises networks. This setup allows for a level of control and customization. This option would involve higher maintenance and management overhead compared to using AWS managed services like Route 53 Resolver, so it is less preferred.
  A: Configure DHCP Options for your VPC to point to the correct AWS solution. Configuring DHCP options to point to an AWS managed DNS solution can help ensure that DNS queries are resolved correctly between AWS and on-premises networks.
  A: Configure the AWS Route 53 Resolver with the needed inbound and outbound resolver rules to handle DNS queries between your VPC and on-premises networks. Route 53 Resolver is designed to handle DNS queries between VPCs and on-premises networks seamlessly. By configuring the necessary inbound and outbound resolver rules, resources in the VPC can resolve both AWS and on-premises DNS resources efficiently. This is a managed solution which reduces operational burden and complexity.

## Network Management and Operation
* Your organization needs to regularly transfer a large amount confidential information from an on-premises location for storage within an S3 bucket. Sustained throughput around 1 Gbps is required. While data confidentiality in transit is a concern, the organizational security department has deemed traffic within an AWS VPC to be sufficiently secure. The reliability of this connection is a high priority. Which implementation best satisfies the requirements?
  A: Use DirectConnect with a Public VIF. Establish a VPN tunnel to an AWS site-to-site VPN. A single EC2-hosted VPN server is a single point of failure. An AWS Site-to-Site VPN Connection provides multi-AZ availability for the VPN endpoints. However, a private VIF cannot connect to an AWS Site-to-Site VPN Connection as the endpoints are in the 'public zone' of the AWS Region. A public VIF must be used to establish tunnels to AWS Site-to-Site VPN Connection endpoints.
* Which of the following are required configurations for an EC2-Hosted VPN server?
  A: Disable source destination check. Since the EC2-hosted VPN server will be forwarding network traffic for other hosts, Source Destination Check must be disabled. While the EC2-hosted VPN server will require a public IP address, that does not have to be provided using an Elastic IP address. Neither enhanced networking or the System Manager agent are required for EC2-hosted VPN servers.
* Your company is planning to deploy a sophisticated application on AWS. The application necessitates a highly specialized operating system configuration and a complex software setup that are not catered to by the default Amazon Machine Images (AMIs). The company aims to initiate operations as swiftly as possible. Which of the following would be the MOST appropriate solution to fulfill these requirements?
  A: Procure an AMI from the AWS Marketplace that aligns with the required configuration. Procuring an AMI from the AWS Marketplace that fulfills the requirements would be an expedient way to meet the company's objectives swiftly.
* You are replacing an EC2-hosted VPN system with an AWS Site-to-Site VPN Connection, using the same customer gateway device at your on-prem network. You decide to use dynamic routing and enable route propagation on all of your VPC route tables. After carefully reconfiguring your customer gateway device using the configuration file downloaded from AWS, you find that while the VPN tunnel can be established with AWS, you are unable to successfully connect with resources in the VPC. What is the most likely cause of this problem?
  A: A conflicting route is present in the VPC route tables. It is most likely that the static route used to point traffic from the EC2 hosted VPN server to the customer gateway is still present in the VPC route tables. If propagated routes from a Site-to-Site VPN connection or AWS DirectConnect connection have the same destination CIDR block as other existing static routes, those static routes are prioritized in most cases.
* Your organization maintains resources in several AWS Regions. They are considering using AWS DirectConnect to access all AWS resources. How can DX fault tolerance be maximized?
  A: Create at least two DX connections to at least two different DX locations in the geographically closest region. Connect a VGW in each regional VPC to a single DX Gateway.
* Your organization maintains resources in several AWS Regions. They are considering using AWS DirectConnect to access all AWS resources. How can DX fault tolerance be maximized?
  A: Create at least two DX connections to at least two different DX locations in the geographically closest region. Connect a VGW in each regional VPC to a single DX Gateway. Create at least two DX connections to at least two different DX locations in the geographically closest region. Connect a Virtual Private Gateway (VGW) in each region to a single DX Gateway. With DirectConnect Gateways, organizations do not need to maintain DX connections to multiple regions. Virtual Private Gateways are attached to VPCs - not regions - and a VPC may only have a single VGW attached at a time. Multiple DX connections to a single DX Location provides some fault tolerance, but not as much as connecting to an additional DX Location as well.
* A corporation is developing its website on AWS within a single VPC, containing public and private subnets across three Availability Zones in the us-west-2 Region. The website utilizes static content, like images, stored in Amazon S3. A group of Amazon EC2 instances acting as web servers was deployed into private subnets. The EC2 instances are within an Auto Scaling group and are fronted by an Application Load Balancer. They serve web traffic and must access content from S3 buckets to create the webpages. The corporation uses AWS Direct Connect with a public VIF for linking its on-premises environment to the S3 bucket. A network engineer discovers that the communication between the EC2 instances and Amazon S3 is being routed via a public NAT gateway. As the traffic grows, the expenses are also growing. The network engineer must update the connectivity to minimize the NAT gateway costs stemming from the traffic between the EC2 instances and Amazon S3. What is the optimal solution to fulfill these needs?
  A: Create and deploy gateway VPC endpoints for Amazon S3. Update the VPC route tables. Implementing gateway VPC endpoints for Amazon S3 is the correct solution. This type of endpoint provides a target for routes in a route table, allowing the traffic between the EC2 instances in the VPC and Amazon S3 to remain entirely within the AWS network, bypassing the NAT gateway. It also does not incur additional charges, thus meeting the requirement to reduce the costs associated with the NAT gateway. By updating the VPC route table to use the gateway VPC endpoint for Amazon S3, the network engineer can redirect traffic from the EC2 instances to the S3 bucket without incurring NAT gateway costs.
* Your organization has an AWS site to site VPN connection established from their office for several months and have had no problems connecting to AWS resources. Recently a new network segment was added at the organizational office. Ever since then, problems connecting to AWS resources from both network segments have been occurring regularly. What is the likely cause of these connection problems?
  A: AWS site-to-site VPN connection tunnels only support a single security association. AWS site-to-site VPN connection tunnels only support a single security association. It is possible that your customer gateway device may be using policy-based VPN rules instead of a single, route-based VPN rule which covers all applicable traffic.
* Your company is planning on creating a DirectConnect connection and also having a VPN as a backup connection. A single router is used for both connections. Which of the following would help establish the DX connection as the preferred path to AWS?
  A: Ensure that the DX path has a longer prefix than the VPN path. If a router has multiple routes that match the intended traffic destination, the route with the longest network ID prefix will be preferred. Where network ID prefixes are the same length, routing protocol manipulation can impact which route becomes the most preferred. With BGP, paths would be preferred depending on higher or lower values of a hierarchy of properties and attributes.
* You are configuring hybrid connectivity to multiple VPCs from your on-prem location. Many of these VPCs have overlapping CIDR blocks. What can be implemented at your on-prem routers to keep the traffic from these overlapping VPCs segregated?
  A: Implement a Virtual Routing and Forwarding solution. Virtual Routing and Forwarding technology allows a routing system to maintain multiple, segregated routing tables, which in turn are associated with physical or logical router interfaces. This allows multiple virtual networks to be hosted on router hardware. In many ways, they are analogous to VLANs, but operate at Layer 3 instead of Layer 2.
* A major branch office of your company requires AWS Direct Connect access to their AWS resources. The Branch and Main offices have different AWS accounts and are connected, but the connectivity hasn't been configured yet. Throughput requirements are estimated to be less than 1 Gbps, and the connectivity needs to be established quickly. The company currently uses a DX Dedicated connection at the main office. Which of the following options will satisfy their requirements at the least expense?
  A: At the parent AWS Account for the Organization, create a Hosted Virtual Interface for the branch office AWS account, using the DX Dedicated connection from the main office. Provided the Branch and Main offices are already connected, creating a Hosted Virtual Interface for the branch organization's AWS account can provide DX access to AWS resources within minutes. The only additional charges would be the DTO charges for the branch office account. DirectConnect Partners set up Hosted Connections, not Hosted VIFS. Only DX Connect partners may set up Hosted Connections, not other AWS accounts. Contacting an AWS DirectConnect partner to help set up a DX Hosted Connection would be more expensive and take more time to establish than the correct answer.

## Network Security, Compliance, and Governance
* You've setup VPC Flow Logs within a subnet, sending the data to CloudWatch logs. Later, when reviewing the logs, you find the following record. The IP address associated with this ENI is 192.168.1.12. What activity does this log entry indicate? 2 433156987611 eni-xyz1357 192.168.1.24 192.168.1.12 53652 22 6 10 1439 1539650419 1539679395 REJECT OK
  A: Traffic was sent to the SSH service on the instance. VPC flow logs sent to cloud watch logs always use the following default format: . Custom VPC flow logs can be sent to CloudWatch Logs or to a S3 bucket. The listed destination IP address is the IP address associated with the ENI that reported this entry, so this is traffic received by the interface. As the listed destination port is TCP 22 - which is the secure shell (SSH) port - this log entry indicates that traffic was sent to the SSH service on the instance.
* A company's network engineer is responsible for creating and evaluating network designs for PCs within their development AWS account. The organization is requiring the ability to track modifications made to different network resources and the ability to enforce rigorous adherence to specific network security protocols. Additionally, the company needs the capability to review the historical configurations of the network resources. What is the solution that will meet these needs?
  A: Begin to record the current state of the network resources by using AWS Config. Then, create Config rules that reflect the desired network resource configuration settings. Set automatic remediation for noncompliant resources. AWS Config provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time. When you add a rule to your account, you can specify when in the resource creation and management process that you want AWS Config to evaluate your resources. The resource creation and management process is known as resource provisioning. You choose the evaluation mode to specify when in this process you want AWS Config to evaluate your resources. You can also choose the trigger type to specify how often your AWS Config rules evaluate your resources. Resources can be evaluated when there are configuration changes, on a periodic schedule, or both. Also, AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents.
* A corporation runs an online e-commerce system that requires encryption of all credit card numbers that are used to complete purchases. The customer-facing application is hosted within Amazon ECS Fargate and is sitting behind an Application Load Balancer (ALB) within the us-east-2 Region and an Amazon CloudFront distribution using the ALB as the origin. The company uses a third-party trusted certificate authority for its public TLS certificates. HTTPS is utilized for encryption in transit within the application, and the company now needs further field-level encryption to maintain the encrypted status of sensitive data during processing. This is meant to ensure that only specific parts of the application have the ability to decrypt this information. What combination of actions will best satisfy these conditions?
  A: Create and then upload a public key that handles the encryption of the chosen sensitive data to the CloudFront distribution. Afterward, make a field-level encryption profile, and specify the fields that contain the sensitive application data. Next, create a field-level encryption configuration, and select the new profile. Finally, configure the appropriate cache behavior to leverage the field encryption configuration. CloudFront field-level encryption uses asymmetric encryption, also known as public key encryption. You provide a public key to CloudFront, and all sensitive data that you specify is encrypted automatically. The key you provide to CloudFront cannot be used to decrypt the encrypted values; only your private key can do that. To use field-level encryption, when you configure your CloudFront distribution, specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request.
  A: Import the third-party certificate for the ALB into AWS Certificate Manager (ACM) in us-east-2. Associate the new certificate with the ALB. Upload the certificate again for the CloudFront distribution into ACM in the us-east-1 Region. To use a certificate in AWS Certificate Manager (ACM) to require HTTPS between viewers and CloudFront, make sure you request (or import) the certificate in the us-east-1 Region. If you want to require HTTPS between CloudFront and your origin, and you are using a load balancer in Elastic Load Balancing as your origin, you can request or import the certificate in any AWS Region. Note: Any imported cert must be in X.509 PEM format. This is the default format if you are using AWS Certificate Manager.
* A corporation is implementing third-party firewall appliances within its VPC to provide traffic analysis and NAT functions. The VPC is set up with both private and public subnets, and the company has a requirement to place the firewall appliances behind a load balancer. What is the MOST cost-effective architecture that will fulfill these specifications?
  A: Create and then deploy a Gateway Load Balancer (GWLB) using the firewall appliances registered within the target group. Configure the firewall appliances with two separate network interfaces: one in a private subnet and the other in a public subnet. Use the built-in NAT functionality to pass the traffic to the internet after the inspection. Gateway Load Balancers enable you to deploy, scale, and manage virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. It combines a transparent network gateway (that is, a single entry and exit point for all traffic) and distributes traffic while scaling your virtual appliances with the demand. A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity between virtual appliances in the service provider VPC and application servers in the service consumer VPC. You deploy the Gateway Load Balancer in the same VPC as the virtual appliances. You register the virtual appliances with a target group for the Gateway Load Balancer. In this case, it is called out that the virtual appliances also perform NAT functionalities, which removes the need for an expensive NAT Gateway. Below are the steps for creating one: Step 1: Create a Gateway Load Balancer. Step 2: Create a Gateway Load Balancer endpoint service. Step 3: Create a Gateway Load Balancer endpoint. Step 4: Configure routing.
* A business operates an application on numerous Amazon EC2 instances. The CISO has recently implemented company requirements that require all network traffic going to and coming from any EC2 instances be directed to a centralized third-party EC2-hosted security appliance for the purpose of content examination. What is the solution that will satisfy these conditions?
  A: Stand up the third-party EC2 security appliance in an Auto Scaling group that is hosted behind a Network Load Balancer. Create a VPC Traffic Mirroring session, and configure the the Network Load Balancer as the mirror target. When setting up the mirroring sessions, specify a mirror filter to capture inbound and outbound traffic. Set the EC2 elastic network interfaces for all the instances that host the application as the source of the mirror session. Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type interface. You can then send the traffic to out-of-band security and monitoring appliances for content inspection, threat monitoring, and troubleshooting. Traffic Mirroring captures duplicate packet information without affecting the performance of the original network traffic. This feature allows for three different target types: network interfaces, Network Load Balancers, and Gateway Load Balancers. Remember, though, that you could experience out-of-order delivery of mirrored packets when you use a Network Load Balancer or Gateway Load Balancer endpoint as your traffic mirror target. A traffic mirror filter is a set of inbound and outbound rules that determines what traffic is copied from the traffic mirror source and sent to the traffic mirror target.
* Your organization is considering setting up an application behind a CloudFront distribution, hosting content on an ALB-fronted origin. Compliance requirements mandate that all steps necessary to mitigate DDoS attacks are to be taken. Which of the following can be implemented to help mitigate these types of attacks?
  A: Activate Shield Advanced and identify resources to protect. You should create two WAF ACLs. A single WAF ACL can be associated with CloudFront distributions, or with either ALBs or API Gateways in a single AWS Region - but can only be associated with one type of service object at a time. Place one WAF ACL in front of the CloudFront distribution and the other in front of the Application Load Balancer. You should also subscribe to Shield Advanced protection and identify resources that you want protected. AWS Shield Advanced provides expanded DDoS attack protection for web applications running on EC2, ELB, CloudFront, Route 53 and Global Accelerator. EC2 instances cannot be protected by AWS WAF ACLs.
  A: Create two WAF ACLs. Place one in front of the CloudFront distribution and the other in front of the Application Load Balancer. You should create two WAF ACLs. A single WAF ACL can be associated with CloudFront distributions, or with either ALBs or API Gateways in a single AWS Region - but can only be associated with one type of service object at a time. Place one WAF ACL in front of the CloudFront distribution and the other in front of the Application Load Balancer. You should also subscribe to Shield Advanced protection and identify resources that you want protected. AWS Shield Advanced provides expanded DDoS attack protection for web applications running on EC2, ELB, CloudFront, Route 53 and Global Accelerator. EC2 instances cannot be protected by AWS WAF ACLs.
* Cloud formation template usage has become the standard means of deploying baseline infrastructure within your organization. However, the ability of sub-department administrators to make changes to deployed infrastructure is increasingly causing problems. In particular, certain dependency resources are being deleted, causing other stack deployments to fail. What can be done to protect select critical resources created using CloudFormation?
  A: Create new IAM policies for sub-department administrator roles that prevent delete actions on dependency objects. Objects created using CloudFormation templates will automatically contain Tags for the object's logical name within the template, as well as the stack's ID and name. IAM policy permissions may be created with conditions referencing any of these tags. CloudFormation stack policies, stack termination protection, and the DeletionPolicy object property only protect objects from being updated or deleted by other CloudFormation stack operations. They do not protect individual objects within a stack from being updated or deleted outside of CloudFormation stack operations.
* Your company is running a service that handles confidential data behind an Elastic Load Balancer. The application transfers data using a proprietary protocol. After a security audit, it is determined that this application requires end-to-end data encryption for all incoming application requests. What must be done to satisfy the security requirement?
  A: Use a classic load balancer. Configure SSL/TLS termination on the EC2 instances. As the application is transferring data using a proprietary protocol, an application load balancer cannot be used, as ALBs only support HTTP and HTTPS listeners. Classic and network load balancers can listen on TCP ports (and UDP on a network load balancer) and forward traffic to the back end clients, which should be configured to terminate the SSL/TLS connection.




  
